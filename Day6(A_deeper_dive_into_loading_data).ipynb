{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8658162,
          "sourceType": "datasetVersion",
          "datasetId": 5187090
        },
        {
          "sourceId": 72225152,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Day6(A deeper dive into loading data)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushanttwayana/PYTHON/blob/main/Day6(A_deeper_dive_into_loading_data).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'water-protability:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5187090%2F8658162%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240611%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240611T173219Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da63fde10ba0bacaa8ff8a2e960bab4415266bfc8098d85c0cb1b08a8b72c04d0482033d2466b2e932994fc938db50fad8215d451aedd1c076215d12daa8ba5a040ecc24a4014a42aa3ed959b265cf2f8710dad22d99c9577e6c93e675ba4af4401f0f5545c550d44517c51cded93cc15ca81368ebfa7168cd15e77516448a5571a42c3983f746b077c9583bc5df46f87015daad32bb8ce0dd7c8fd808dc1db14715e319a1cd095a742bc8e74ae14e436ef6623f3b4c5988d4ab1133ac55b8d7aabf5fbcb1165ad7903bc0421ee036da2431fff3f900c9c2015dd8be9a84255bd883de5ab5433232bf17ac1191241f1e8f58010be21a0d00456a2dab07dc7e280'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "uMa2LW-zW8GJ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the TensorDataset class\n",
        "\n",
        "In practice, loading your data into a PyTorch dataset will be one of the first steps you take in order to create and train a neural network with PyTorch.\n",
        "\n",
        "The TensorDataset class is very helpful when your dataset can be loaded directly as a NumPy array. Recall that TensorDataset() can take one or more NumPy arrays as input.\n",
        "\n",
        "In this exercise, you'll practice creating a PyTorch dataset using the TensorDataset class.\n",
        "\n",
        "torch and numpy have already been imported for you, along with the TensorDataset class."
      ],
      "metadata": {
        "id": "wOOXKcgGW8GO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-10T18:11:05.146688Z",
          "iopub.execute_input": "2024-06-10T18:11:05.147998Z",
          "iopub.status.idle": "2024-06-10T18:11:05.153678Z",
          "shell.execute_reply.started": "2024-06-10T18:11:05.147947Z",
          "shell.execute_reply": "2024-06-10T18:11:05.152457Z"
        },
        "trusted": true,
        "id": "kIYv17viW8GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "np_features = np.array(np.random.rand(12, 8))\n",
        "np_target = np.array(np.random.rand(12, 1))\n",
        "\n",
        "# Convert arrays to PyTorch tensors\n",
        "torch_features = torch.tensor(np_features)\n",
        "torch_target = torch.tensor(np_target)\n",
        "# Create a TensorDataset from two tensors\n",
        "dataset = TensorDataset(torch_features, torch_target)\n",
        "# dataset = TensorDataset(torch_features.float(), torch_target.float())\n",
        "\n",
        "# Return the last element of this dataset\n",
        "print(dataset[-1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-10T18:09:27.392653Z",
          "iopub.execute_input": "2024-06-10T18:09:27.393117Z",
          "iopub.status.idle": "2024-06-10T18:09:27.406191Z",
          "shell.execute_reply.started": "2024-06-10T18:09:27.393084Z",
          "shell.execute_reply": "2024-06-10T18:09:27.404564Z"
        },
        "trusted": true,
        "id": "N9DJgLcbW8GR",
        "outputId": "3e4acbe6-e2dd-4a72-8a58-6d5d01150e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "(tensor([0.7421, 0.5156, 0.8350, 0.6717, 0.2516, 0.8435, 0.3683, 0.8443],\n       dtype=torch.float64), tensor([0.0609], dtype=torch.float64))\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorDataset is great to use when your dataset can be loaded from NumPy arrays (or converted to NumPy arrays). However, sometimes you need to code a custom dataset class.\n",
        "\n"
      ],
      "metadata": {
        "id": "wg10SyJqW8GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.read_csv(\"/kaggle/input/water-protability/water_potability.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-10T18:10:07.397984Z",
          "iopub.execute_input": "2024-06-10T18:10:07.398502Z",
          "iopub.status.idle": "2024-06-10T18:10:07.439595Z",
          "shell.execute_reply.started": "2024-06-10T18:10:07.398428Z",
          "shell.execute_reply": "2024-06-10T18:10:07.438269Z"
        },
        "trusted": true,
        "id": "6pitUxk8W8GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.DataFrame({\n",
        "    'ph': [7.0, 8.1, np.nan, 7.8],\n",
        "    'Sulfate': [300, 320, 330, np.inf],\n",
        "    'Solids': [20000, 21000, 22000, 23000],\n",
        "    'Conductivity': [400, 420, 430, 440],\n",
        "    'Chloramines': [3.1, 3.2, 3.3, 3.4],\n",
        "    'Turbidity': [4.0, 4.1, 4.2, 4.3],\n",
        "    'Hardness': [150, 160, 170, 180],\n",
        "    'Organic_carbon': [10, 11, 12, 13],\n",
        "    'Potability': [0, 1, 0, 1]\n",
        "})\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-10T18:10:09.760811Z",
          "iopub.execute_input": "2024-06-10T18:10:09.761358Z",
          "iopub.status.idle": "2024-06-10T18:10:09.77308Z",
          "shell.execute_reply.started": "2024-06-10T18:10:09.761318Z",
          "shell.execute_reply": "2024-06-10T18:10:09.771117Z"
        },
        "trusted": true,
        "id": "4nXlT_WPW8GU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the features\n",
        "features = dataframe[['ph', 'Sulfate', 'Solids', 'Conductivity', 'Chloramines', 'Turbidity', 'Hardness', 'Organic_carbon']]\n",
        "features = (features - features.mean()) / features.std()\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "features_tensor = torch.tensor(features.to_numpy()).float()\n",
        "target_tensor = torch.tensor(dataframe['Potability'].to_numpy()).float()\n",
        "\n",
        "# Create a dataset from the two generated tensors\n",
        "dataset = TensorDataset(features_tensor, target_tensor)\n",
        "\n",
        "# Create a dataloader using the above dataset\n",
        "dataloader = DataLoader(dataset, shuffle=True, batch_size=2)\n",
        "\n",
        "# Create a model using the nn.Sequential API\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(8, 16),  # Adjust the input dimension to 8 to match the features\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 1),\n",
        "    nn.Sigmoid()  # Sigmoid activation function to squash output values to [0, 1]\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for features_batch, target_batch in dataloader:\n",
        "        # Forward pass\n",
        "        output = model(features_batch)\n",
        "\n",
        "        # Debugging: print output values\n",
        "        print(f\"Output: {output.detach().numpy()}\")\n",
        "\n",
        "        # Ensure target shape matches output\n",
        "        loss = criterion(output, target_batch.unsqueeze(1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# After training, let's print out the output of the trained model\n",
        "output = model(features_tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-10T18:10:11.934912Z",
          "iopub.execute_input": "2024-06-10T18:10:11.935359Z",
          "iopub.status.idle": "2024-06-10T18:10:12.099279Z",
          "shell.execute_reply.started": "2024-06-10T18:10:11.935317Z",
          "shell.execute_reply": "2024-06-10T18:10:12.096628Z"
        },
        "trusted": true,
        "id": "677Yos2RW8GV",
        "outputId": "c536da17-4cb4-4817-b4e9-a60561a86c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Output: [[nan]\n [nan]]\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n  sqr = _ensure_numeric((avg - values) ** 2)\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Ensure target shape matches output\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3122\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3119\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3120\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
          ],
          "ename": "RuntimeError",
          "evalue": "all elements of input should be between 0 and 1",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating model performance\n",
        "\n",
        "**Writing the evaluation loop**\n",
        "\n",
        "In this exercise, you will practice writing the evaluation loop. Recall that the evaluation loop is similar to the training loop, except that you will not perform the gradient calculation and the optimizer step.\n",
        "\n",
        "The model has already been defined for you, along with the object validationloader, which is a dataset."
      ],
      "metadata": {
        "id": "XwshLhO7W8GW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2RZNEQtlW8GX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "validation_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for data in validationloader:\n",
        "\n",
        "          outputs = model(data[0])\n",
        "          loss = criterion(outputs, data[1])\n",
        "\n",
        "          # Sum the current loss to the validation_loss variable\n",
        "          validation_loss += loss.item()\n",
        "\n",
        "# Calculate the mean loss value\n",
        "validation_loss_epoch = validation_loss / len(validationloader)\n",
        "print(validation_loss_epoch)\n",
        "\n",
        "# Set the model back to training mode\n",
        "model.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-10T18:10:25.642547Z",
          "iopub.execute_input": "2024-06-10T18:10:25.643117Z",
          "iopub.status.idle": "2024-06-10T18:10:25.7017Z",
          "shell.execute_reply.started": "2024-06-10T18:10:25.643076Z",
          "shell.execute_reply": "2024-06-10T18:10:25.700174Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "nSqoLjVbW8GX",
        "outputId": "60abb37b-bae9-4943-8b66-76769b30e799"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m validation_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvalidationloader\u001b[49m:\n\u001b[1;32m      9\u001b[0m           outputs \u001b[38;5;241m=\u001b[39m model(data[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     10\u001b[0m           loss \u001b[38;5;241m=\u001b[39m criterion(outputs, data[\u001b[38;5;241m1\u001b[39m])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'validationloader' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'validationloader' is not defined",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:4a0d6b36-36cd-4374-a36c-f57cf66e0884.png)"
      ],
      "metadata": {
        "id": "EXk7Yv6yW8GY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating accuracy using torchmetrics**\n",
        "In addition to the losses, you should also be keeping track of the accuracy during training. By doing so, you will be able to select the epoch when the model performed the best.\n",
        "\n",
        "In this exercise, you will practice using the torchmetrics package to calculate the accuracy. You will be using a sample of the facemask dataset. This dataset contains three different classes. The plot_errors function will display samples where the model predictions do not match the ground truth. Performing such error analysis will help you understand your model failure modes.\n",
        "\n",
        "The torchmetrics package is already imported. The model outputs are the probabilities returned by a softmax as the last step of the model. The labels tensor contains the labels as one-hot encoded vectors."
      ],
      "metadata": {
        "id": "u98wXne-W8GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-10T18:16:31.224252Z",
          "iopub.execute_input": "2024-06-10T18:16:31.224914Z",
          "iopub.status.idle": "2024-06-10T18:16:33.841462Z",
          "shell.execute_reply.started": "2024-06-10T18:16:31.224871Z",
          "shell.execute_reply": "2024-06-10T18:16:33.840368Z"
        },
        "trusted": true,
        "id": "qgp0skqFW8GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create accuracy metric using torch metrics\n",
        "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
        "for data in dataloader:\n",
        "    features, labels = data\n",
        "    outputs = model(features)\n",
        "\n",
        "    # Calculate accuracy over the batch\n",
        "    acc = metric(outputs.softmax(dim=-1), labels.argmax(dim=-1))\n",
        "\n",
        "# Calculate accuracy over the whole epoch\n",
        "acc = metric.compute()\n",
        "\n",
        "# Reset the metric for the next epoch\n",
        "model.reset()\n",
        "plot_errors(model, dataloader)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-10T18:16:39.694737Z",
          "iopub.execute_input": "2024-06-10T18:16:39.695457Z",
          "iopub.status.idle": "2024-06-10T18:16:40.281662Z",
          "shell.execute_reply.started": "2024-06-10T18:16:39.69541Z",
          "shell.execute_reply": "2024-06-10T18:16:40.279494Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "AVHN4sMwW8GY",
        "outputId": "4dee2398-dd51-4f56-feba-506004f3fc31"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Calculate accuracy over the batch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     acc \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate accuracy over the whole epoch\u001b[39;00m\n\u001b[1;32m     11\u001b[0m acc \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcompute()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchmetrics/metric.py:311\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchmetrics/metric.py:380\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchmetrics/metric.py:482\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 482\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchmetrics/classification/stat_scores.py:339\u001b[0m, in \u001b[0;36mMulticlassStatScores.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_args:\n\u001b[0;32m--> 339\u001b[0m     \u001b[43m_multiclass_stat_scores_tensor_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_format(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[1;32m    343\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_update(\n\u001b[1;32m    344\u001b[0m     preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[1;32m    345\u001b[0m )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchmetrics/functional/classification/stat_scores.py:310\u001b[0m, in \u001b[0;36m_multiclass_stat_scores_tensor_validation\u001b[0;34m(preds, target, num_classes, multidim_average, ignore_index)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `preds` and `target` have the same shape, the shape of `preds` should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m at least 2D when multidim_average is set to `samplewise`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither `preds` and `target` both should have the (same) shape (N, ...), or `target` should be (N, ...)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and `preds` should be (N, C, ...).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    315\u001b[0m check_value \u001b[38;5;241m=\u001b[39m num_classes \u001b[38;5;28;01mif\u001b[39;00m ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_classes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, name \u001b[38;5;129;01min\u001b[39;00m ((target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m),) \u001b[38;5;241m+\u001b[39m ((preds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m),) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m ():  \u001b[38;5;66;03m# noqa: RUF005\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: Either `preds` and `target` both should have the (same) shape (N, ...), or `target` should be (N, ...) and `preds` should be (N, C, ...)."
          ],
          "ename": "ValueError",
          "evalue": "Either `preds` and `target` both should have the (same) shape (N, ...), or `target` should be (N, ...) and `preds` should be (N, C, ...).",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy is a great metric for classification problems. Calculating the class-wise accuracy gives a better understanding of your model performances. Moreover, by looking at your model misclassification, you can find trends in the errors and better understand when your model fails."
      ],
      "metadata": {
        "id": "EvnjLtFFW8GZ"
      }
    }
  ]
}